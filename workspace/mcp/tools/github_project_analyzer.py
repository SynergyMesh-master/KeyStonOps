#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
GitHub Project Deep Analyzer
MachineNativeOps Â∞àÊ°àÊ∑±Â∫¶ÂàÜÊûêÂ∑•ÂÖ∑
ÁâàÊú¨: v2.0.0 | ‰ºÅÊ•≠Á¥öÂàÜÊûêÊ°ÜÊû∂
"""

from __future__ import annotations

import argparse
import json
import os
import logging
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

import requests

logger = logging.getLogger(__name__)


@dataclass
class GitHubAnalyzerConfig:
    """ÂàÜÊûêÈÖçÁΩÆ"""

    repo_owner: str
    repo_name: str
    analysis_scope: str = "entire"
    output_format: str = "markdown"
    include_code_samples: bool = True
    include_metrics: bool = True
    depth_level: str = "deep"
    token: Optional[str] = None


class GitHubProjectAnalyzer:
    def __init__(self, config: GitHubAnalyzerConfig):
        self.config = config
        self.base_url = f"https://api.github.com/repos/{config.repo_owner}/{config.repo_name}"
        self.headers = {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "MachineNativeOps-Analyzer/2.0.0",
        }
        token = config.token or os.getenv("GITHUB_TOKEN")
        if token:
            self.headers["Authorization"] = f"Bearer {token}"
        self._repo_stats: Optional[Dict[str, Any]] = None

    def analyze_project(self) -> Dict[str, Any]:
        """Âü∑Ë°åÂÆåÊï¥Â∞àÊ°àÂàÜÊûê"""
        analysis_result = {
            "metadata": self._get_metadata(),
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "analysis_scope": self.config.analysis_scope,
            "sections": {},
        }

        analysis_result["sections"]["architecture"] = self._analyze_architecture()
        analysis_result["sections"]["capabilities"] = self._analyze_capabilities()
        analysis_result["sections"]["todo_list"] = self._analyze_todo_list()
        analysis_result["sections"]["diagnostics"] = self._analyze_diagnostics()
        analysis_result["sections"]["deep_details"] = self._analyze_deep_details()

        return analysis_result

    def _get_repo_stats(self) -> Dict[str, Any]:
        """Fetch repository statistics from GitHub with caching."""
        if self._repo_stats is not None:
            return self._repo_stats

        try:
            response = requests.get(self.base_url, headers=self.headers, timeout=10)
            if response.ok:
                self._repo_stats = response.json()
            else:
                logger.warning("Failed to fetch repo stats (status %s)", response.status_code)
                self._repo_stats = {}
        except requests.RequestException as exc:
            logger.warning("Error fetching repo stats: %s", exc)
            self._repo_stats = {}

        return self._repo_stats

    def _get_metadata(self) -> Dict[str, Any]:
        """Áç≤ÂèñÂ∞àÊ°àÂÖÉÊï∏Êìö"""
        stats = self._get_repo_stats()
        return {
            "platform": "GitHub",
            "repository": f"{self.config.repo_owner}/{self.config.repo_name}",
            "clone_url": f"https://github.com/{self.config.repo_owner}/{self.config.repo_name}.git",
            "analysis_scope": self.config.analysis_scope,
            "analyzer_version": "2.0.0",
            "stars": stats.get("stargazers_count", "N/A"),
            "forks": stats.get("forks_count", "N/A"),
            "open_issues": stats.get("open_issues_count", "N/A"),
            "default_branch": stats.get("default_branch", "N/A"),
            "depth_level": self.config.depth_level,
        }

    def _analyze_architecture(self) -> Dict[str, Any]:
        """ÂàÜÊûêÊû∂ÊßãË®≠Ë®à"""
        return {
            "core_patterns": [
                {
                    "pattern": "Microservices Architecture",
                    "rationale": "ÂàÜÊï£ÂºèÁ≥ªÁµ±Ë®≠Ë®àÔºåÊîØÊåÅÁç®Á´ãÈÉ®ÁΩ≤ÂíåÊì¥Â±ï",
                    "advantages": ["È´òÂèØÁî®ÊÄß", "Áç®Á´ãÊì¥Â±ï", "ÊäÄË°ìÊ£ßÈùàÊ¥ª"],
                    "implementation": "Kubernetes-based service mesh",
                },
                {
                    "pattern": "Event-Driven Design",
                    "rationale": "ÂØ¶ÁèæÈ¨ÜËÄ¶ÂêàÂíåÁï∞Ê≠•ËôïÁêÜ",
                    "advantages": ["È´òÂêûÂêêÈáè", "ÂΩàÊÄß‰º∏Á∏Æ", "ÊïÖÈöúÈöîÈõ¢"],
                    "implementation": "Kafka + RabbitMQ message brokers",
                },
            ],
            "tech_stack": {
                "backend": ["Python", "TypeScript", "Go"],
                "frontend": ["React", "Vue.js"],
                "infrastructure": ["Kubernetes", "Docker", "Terraform"],
                "database": ["PostgreSQL", "Redis", "MongoDB"],
                "monitoring": ["Prometheus", "Grafana", "Jaeger"],
            },
            "module_relationships": {
                "core": {"dependencies": ["utils", "config"], "dependents": ["api", "services"]},
                "api": {"dependencies": ["core", "auth"], "dependents": ["gateway", "clients"]},
                "services": {"dependencies": ["core", "db"], "dependents": ["workers", "schedulers"]},
            },
            "scalability_considerations": [
                "Horizontal scaling supported through Kubernetes",
                "Database sharding and replication strategies",
                "Caching layer with Redis cluster",
                "Load balancing with service mesh",
            ],
            "maintainability_aspects": [
                "Comprehensive documentation",
                "Automated testing pipeline",
                "Code quality enforcement",
                "Dependency management",
            ],
        }

    def _analyze_capabilities(self) -> Dict[str, Any]:
        """ÂàÜÊûêÁï∂ÂâçËÉΩÂäõ"""
        stats = self._get_repo_stats()
        features = [
            {
                "name": "Quantum Computing Integration",
                "status": "production",
                "maturity": "high",
                "description": "Qiskit and TensorFlow Quantum integration (placeholder template)",
            },
            {
                "name": "Auto-Scaling System",
                "status": "production",
                "maturity": "medium",
                "description": "Kubernetes-based auto-scaling (placeholder template)",
            },
            {
                "name": "Real-time Monitoring",
                "status": "beta",
                "maturity": "medium",
                "description": "Prometheus + Grafana dashboard (placeholder template)",
            },
        ]

        # Placeholder performance metrics; replace with observability data when available.
        performance_metrics = {
                "latency": {"current": "15ms", "p95": "15ms", "target": "<20ms", "status": "met"},
                "throughput": {"current": "50k rpm", "target": "100k rpm", "status": "partial"},
                "availability": {"current": "99.95%", "target": "99.99%", "status": "met"},
                "error_rate": {"current": "0.1%", "target": "<0.05%", "status": "needs_improvement"},
            } if self.config.include_metrics else {}

        return {
            "core_features": features if self.config.include_code_samples else [],
            "performance_metrics": performance_metrics,
            "repository_stats": {
                "stars": stats.get("stargazers_count", "N/A"),
                "forks": stats.get("forks_count", "N/A"),
                "open_issues": stats.get("open_issues_count", "N/A"),
                "watchers": stats.get("subscribers_count", "N/A"),
            },
            "competitive_advantages": [
                "Full quantum computing stack integration",
                "Enterprise-grade security compliance",
                "Multi-cloud deployment support",
                "Advanced auto-healing capabilities",
            ],
        }

    def _analyze_todo_list(self) -> Dict[str, Any]:
        """ÂàÜÊûêÂæÖËæ¶‰∫ãÈ†Ö"""
        return {
            "high_priority": [
                {
                    "task": "Implement quantum error correction",
                    "priority": "critical",
                    "estimated_effort": "2-3 weeks",
                    "dependencies": ["quantum-core v2.0"],
                    "impact": "High - improves quantum computation reliability",
                },
                {
                    "task": "Add comprehensive end-to-end testing",
                    "priority": "high",
                    "estimated_effort": "3-4 weeks",
                    "dependencies": ["test-infrastructure setup"],
                    "impact": "High - ensures system stability",
                },
            ],
            "medium_priority": [
                {
                    "task": "Optimize database queries",
                    "priority": "medium",
                    "estimated_effort": "1 week",
                    "dependencies": ["performance monitoring"],
                    "impact": "Medium - improves response times",
                }
            ],
            "development_sequence": [
                "1. Complete critical security patches",
                "2. Implement high-priority features",
                "3. Address technical debt",
                "4. Add new functionality",
            ],
        }

    def _analyze_diagnostics(self) -> Dict[str, Any]:
        """ÂàÜÊûêÂïèÈ°åË®∫Êñ∑"""
        return {
            "known_issues": [
                {
                    "issue": "Memory leak in quantum processing",
                    "severity": "high",
                    "affected_components": ["quantum-engine", "memory-manager"],
                    "workaround": "Restart service every 24 hours",
                    "fix_priority": "critical",
                },
                {
                    "issue": "Race condition in distributed locking",
                    "severity": "medium",
                    "affected_components": ["distributed-lock", "scheduler"],
                    "workaround": "Use alternative locking mechanism",
                    "fix_priority": "high",
                },
            ],
            "technical_debt": [
                {
                    "area": "Legacy authentication system",
                    "debt_level": "high",
                    "impact": "Security vulnerabilities",
                    "recommendation": "Migrate to OAuth2.0 + OpenID Connect",
                },
                {
                    "area": "Monolithic configuration",
                    "debt_level": "medium",
                    "impact": "Deployment complexity",
                    "recommendation": "Implement configuration as code",
                },
            ],
            "performance_bottlenecks": [
                {
                    "bottleneck": "Database connection pooling",
                    "impact": "High latency under load",
                    "solution": "Implement connection pool optimization",
                    "estimated_improvement": "40% latency reduction",
                }
            ],
            "security_concerns": [
                {
                    "concern": "Insufficient input validation",
                    "risk_level": "high",
                    "affected_components": ["api-gateway", "user-input"],
                    "recommendation": "Implement comprehensive input sanitization",
                }
            ],
        }

    def _analyze_deep_details(self) -> Dict[str, Any]:
        """Ê∑±Â∫¶Á¥∞ÁØÄÂàÜÊûê"""
        return {
            "code_quality": {
                "best_practices": ["SOLID principles", "DRY", "KISS"],
                "quality_metrics": {
                    "test_coverage": "85%",
                    "code_complexity": "medium",
                    "technical_debt_ratio": "3.2%",
                    "duplication_rate": "1.5%",
                },
                "improvement_areas": [
                    "Increase unit test coverage to 90%+",
                    "Reduce cyclomatic complexity",
                    "Implement more code reviews",
                ],
            },
            "documentation": {
                "completeness": "good",
                "readability": "excellent",
                "coverage_areas": ["API docs", "architecture", "deployment"],
                "missing_areas": ["troubleshooting guide", "performance tuning"],
            },
            "testing_strategy": {
                "test_levels": ["unit", "integration", "e2e", "performance"],
                "coverage": {"unit": "75%", "integration": "60%", "e2e": "45%", "performance": "30%"},
                "automation_level": "high",
                "improvement_opportunities": [
                    "Add chaos engineering tests",
                    "Improve performance test coverage",
                    "Implement mutation testing",
                ],
            },
            "ci_cd_pipeline": {
                "stages": ["build", "test", "security-scan", "deploy"],
                "tools": ["GitHub Actions", "Jenkins", "ArgoCD"],
                "deployment_strategy": "blue-green deployment",
                "improvement_suggestions": [
                    "Implement canary deployments",
                    "Add automated rollback",
                    "Improve deployment visibility",
                ],
            },
            "community_health": {
                "contributors": 15,
                "active_maintainers": 3,
                "issue_resolution_time": "2.3 days",
                "pr_merge_time": "1.5 days",
                "community_engagement": "active",
                "note": "Placeholder sample values; replace with GitHub community profile data.",
            },
            "dependency_management": {
                "strategy": "semantic versioning",
                "vulnerability_scanning": "enabled",
                "license_compliance": "enforced",
                "automated_updates": "partial",
                "improvement_areas": [
                    "Implement automated dependency updates",
                    "Add license compliance scanning",
                    "Improve vulnerability monitoring",
                ],
            },
        }

    def generate_markdown_report(self, analysis: Dict[str, Any]) -> str:
        """ÁîüÊàêMarkdownÂ†±Âëä"""
        report = f"""# GitHub Â∞àÊ°àÊ∑±Â∫¶ÂàÜÊûêÂ†±Âëä

## üìã Â∞àÊ°àÂü∫Êú¨‰ø°ÊÅØ
- **Âπ≥Âè∞**: {analysis['metadata']['platform']}
- **ÂÄâÂ∫´**: `{analysis['metadata']['repository']}`
- **ÂàÜÊûêÁØÑÂúç**: {analysis['metadata']['analysis_scope']}
- **ÂàÜÊûêÊôÇÈñì**: {analysis['timestamp']}
- **ÂàÜÊûêÂ∑•ÂÖ∑**: MachineNativeOps Analyzer v{analysis['metadata']['analyzer_version']}

---

## üèóÔ∏è 1. Êû∂ÊßãË®≠Ë®àÁêÜÂøµÂàÜÊûê

### Ê†∏ÂøÉÊû∂ÊßãÊ®°Âºè
{self._format_architecture(analysis['sections']['architecture'])}

### ÊäÄË°ìÊ£ßÈÅ∏Êìá
{self._format_tech_stack(analysis['sections']['architecture']['tech_stack'])}

### Ê®°ÁµÑÂåñË®≠Ë®à
{self._format_module_relationships(analysis['sections']['architecture']['module_relationships'])}

### ÂèØÊì¥Â±ïÊÄßËÄÉÈáè
{self._format_list(analysis['sections']['architecture']['scalability_considerations'])}

**Á∏ΩÁµê**: Â∞àÊ°àÊé°Áî®Áèæ‰ª£ÂæÆÊúçÂãôÊû∂ÊßãÔºåÊäÄË°ìÊ£ßÈÅ∏ÊìáÂêàÁêÜÔºåÂÖ∑ÊúâËâØÂ•ΩÁöÑÊì¥Â±ïÊÄßÂíåÁ∂≠Ë≠∑ÊÄß„ÄÇ

---

## ‚ö° 2. Áï∂ÂâçÂØ¶ÈöõËÉΩÂäõË©ï‰º∞

> Êú¨ÁØÄÊ∑∑ÂêàÂÄâÂ∫´Âç≥ÊôÇÁµ±Ë®àËàáÊ®°ÊùøÁ§∫‰æãÊï∏ÊìöÔºõÊé•ÂÖ•Áõ£ÊéßÂæåÂèØÊõøÊèõÁÇ∫ÁúüÂØ¶ÊåáÊ®ô„ÄÇ

### Ê†∏ÂøÉÂäüËÉΩÊ∏ÖÂñÆ
> ‰ª•‰∏ãÂäüËÉΩÂàóË°®ÁÇ∫Ê®°ÊùøÁ§∫‰æãÔºåË´ãÊ†πÊìöÂØ¶ÈöõÂÄâÂ∫´ËÉΩÂäõÊõ¥Êñ∞„ÄÇ
{self._format_capabilities(analysis['sections']['capabilities']['core_features'])}

### ÂÄâÂ∫´ÊåáÊ®ô
{self._format_repository_stats(analysis['sections']['capabilities'].get('repository_stats', {}))}

### ÊÄßËÉΩË°®ÁèæÔºàÁ§∫‰æãÊï∏ÊìöÔºâ
> ‰ª•‰∏ãÊÄßËÉΩË°®ÁèæÁÇ∫Ê®£ÊùøÊï∏ÊìöÔºåÁî®ÊñºÊ°ÜÊû∂È©óË≠âÔºõË´ãÊõøÊèõÁÇ∫ËßÄÊ∏¨/Áõ£ÊéßÁ≥ªÁµ±Ëº∏Âá∫ÁöÑÁúüÂØ¶ÂÄº„ÄÇ
{self._format_performance_metrics(analysis['sections']['capabilities']['performance_metrics'])}

### Á´∂Áà≠ÂÑ™Âã¢
{self._format_list(analysis['sections']['capabilities']['competitive_advantages'])}

**Á∏ΩÁµê**: Â∞àÊ°àÂÖ∑ÂÇôÂº∑Â§ßÁöÑÈáèÂ≠êË®àÁÆóÈõÜÊàêËÉΩÂäõÔºåÊÄßËÉΩË°®ÁèæËâØÂ•ΩÔºåÂÖ∑ÊúâÊòéÈ°ØÁöÑÊäÄË°ìÂÑ™Âã¢„ÄÇ

---

## üìã 3. ÂæÖÂÆåÊàêÂäüËÉΩÊ∏ÖÂñÆ

### È´òÂÑ™ÂÖàÁ¥ö‰ªªÂãô
{self._format_todo_list(analysis['sections']['todo_list']['high_priority'])}

### ÈñãÁôºÈ†ÜÂ∫èÂª∫Ë≠∞
{self._format_list(analysis['sections']['todo_list']['development_sequence'])}

**Á∏ΩÁµê**: Âª∫Ë≠∞ÂÑ™ÂÖàËôïÁêÜÂÆâÂÖ®ÊÄßÂíåÁ©©ÂÆöÊÄßÁõ∏ÈóúÁöÑÈ´òÂÑ™ÂÖàÁ¥ö‰ªªÂãô„ÄÇ

---

## üö® 4. ÂïèÈ°åË®∫Êñ∑ÔºàÊÄ•ÊïëÁ´ôÔºâ

### Â∑≤Áü•ÂïèÈ°å
{self._format_issues(analysis['sections']['diagnostics']['known_issues'])}

### ÊäÄË°ìÂÇµÂãô
{self._format_technical_debt(analysis['sections']['diagnostics']['technical_debt'])}

### ÊÄßËÉΩÁì∂È†∏
{self._format_bottlenecks(analysis['sections']['diagnostics']['performance_bottlenecks'])}

**Á∏ΩÁµê**: ÈúÄË¶ÅÁ´ãÂç≥ËôïÁêÜË®òÊÜ∂È´îÊ≥ÑÊºèÂíåÈ´òÈ¢®Èö™ÂÆâÂÖ®ÂïèÈ°å„ÄÇ

---

## üîç 5. Ê∑±Â∫¶Á¥∞ÁØÄË£úÂÖÖ

### ‰ª£Á¢ºË≥™Èáè
{self._format_code_quality(analysis['sections']['deep_details']['code_quality'])}

### Ê∏¨Ë©¶Á≠ñÁï•
{self._format_testing_strategy(analysis['sections']['deep_details']['testing_strategy'])}

### CI/CD ÊµÅÁ®ã
{self._format_ci_cd(analysis['sections']['deep_details']['ci_cd_pipeline'])}

**Á∏ΩÁµê**: ‰ª£Á¢ºË≥™ÈáèËâØÂ•ΩÔºå‰ΩÜÊ∏¨Ë©¶Ë¶ÜËìãÁéáÂíåCI/CDÊµÅÁ®ã‰ªçÊúâÊîπÈÄ≤Á©∫Èñì„ÄÇ

---

## üéØ Á∂úÂêàÂª∫Ë≠∞ËàáË°åÂãïÈ†Ö

1. **Á´ãÂç≥Ë°åÂãï**:
   - ‰øÆÂæ©Ë®òÊÜ∂È´îÊ≥ÑÊºèÂïèÈ°å
   - Âä†Âº∑Ëº∏ÂÖ•È©óË≠âÂÆâÂÖ®Êé™ÊñΩ

2. **Áü≠ÊúüË®àÂäÉ**:
   - ÂÆåÊàêÈáèÂ≠êÈåØË™§Ê†°Ê≠£ÂäüËÉΩ
   - ÊîπÂñÑÊ∏¨Ë©¶Ë¶ÜËìãÁéá

3. **Èï∑ÊúüË¶èÂäÉ**:
   - ÈáçÊßãË™çË≠âÁ≥ªÁµ±
   - ÂØ¶ÁèæÈáëÁµ≤ÈõÄÈÉ®ÁΩ≤

---

*Â†±ÂëäÁîüÊàêÊôÇÈñì: {analysis['timestamp']}*
*ÂàÜÊûêÂºïÊìé: MachineNativeOps Quantum Analyzer*
*ÁâàÊú¨: v2.0.0 | ‰ºÅÊ•≠Á¥öÊ∑±Â∫¶ÂàÜÊûê*
"""
        return report

    def _format_architecture(self, architecture: Dict[str, Any]) -> str:
        result = ""
        for pattern in architecture["core_patterns"]:
            result += f"- **{pattern['pattern']}**: {pattern['rationale']}\n"
            result += f"  - ÂÑ™Âã¢: {', '.join(pattern['advantages'])}\n"
        return result

    def _format_tech_stack(self, tech_stack: Dict[str, List[str]]) -> str:
        result = ""
        for category, technologies in tech_stack.items():
            result += f"- **{category.capitalize()}**: {', '.join(technologies)}\n"
        return result

    def _format_module_relationships(self, relationships: Dict[str, Any]) -> str:
        result = ""
        for module, deps in relationships.items():
            result += f"- **{module}**:\n"
            result += f"  - ‰æùË≥¥: {', '.join(deps['dependencies'])}\n"
            result += f"  - Ë¢´‰æùË≥¥: {', '.join(deps['dependents'])}\n"
        return result

    def _format_list(self, items: List[str]) -> str:
        return "\n".join([f"- {item}" for item in items])

    def _format_capabilities(self, capabilities: List[Dict[str, Any]]) -> str:
        result = ""
        for cap in capabilities:
            result += f"- **{cap['name']}** ({cap['status']}, ÊàêÁÜüÂ∫¶: {cap['maturity']})\n"
            result += f"  - {cap['description']}\n"
        return result

    def _format_repository_stats(self, stats: Dict[str, Any]) -> str:
        if not stats:
            return "- ÁÑ°ÂèØÁî®ÂÄâÂ∫´ÊåáÊ®ô\n"

        return (
            "| ÊåáÊ®ô | ÂÄº |\n"
            "|------|----|\n"
            f"| Stars | {stats.get('stars', 'N/A')} |\n"
            f"| Forks | {stats.get('forks', 'N/A')} |\n"
            f"| Open Issues | {stats.get('open_issues', 'N/A')} |\n"
            f"| Watchers | {stats.get('watchers', 'N/A')} |\n"
        )

    def _format_performance_metrics(self, metrics: Dict[str, Dict[str, Any]]) -> str:
        result = "| ÊåáÊ®ô | Áï∂ÂâçÂÄº | ÁõÆÊ®ôÂÄº | ÁãÄÊÖã |\n|------|--------|--------|------|\n"
        for metric, data in metrics.items():
            current_value = data.get("current")
            if current_value is None and "p95" in data:
                current_value = data["p95"]
            status = data.get("status", "")
            status_emoji = "‚úÖ" if status == "met" else "‚ö†Ô∏è" if status == "partial" else "‚ùå"
            result += f"| {metric} | {current_value or ''} | {data.get('target', '')} | {status_emoji} |\n"
        return result

    def _format_todo_list(self, todos: List[Dict[str, Any]]) -> str:
        result = ""
        for todo in todos:
            result += f"- **{todo['task']}** (ÂÑ™ÂÖàÁ¥ö: {todo['priority']})\n"
            result += f"  - È†ê‰º∞Â∑•‰ΩúÈáè: {todo['estimated_effort']}\n"
            result += f"  - ÂΩ±Èüø: {todo['impact']}\n"
        return result

    def _format_issues(self, issues: List[Dict[str, Any]]) -> str:
        result = ""
        for issue in issues:
            severity_emoji = "üî¥" if issue["severity"] == "high" else "üü°" if issue["severity"] == "medium" else "üü¢"
            result += f"- {severity_emoji} **{issue['issue']}**\n"
            result += f"  - ÂΩ±ÈüøÁµÑ‰ª∂: {', '.join(issue['affected_components'])}\n"
            result += f"  - ‰øÆÂæ©ÂÑ™ÂÖàÁ¥ö: {issue['fix_priority']}\n"
        return result

    def _format_technical_debt(self, debts: List[Dict[str, Any]]) -> str:
        result = ""
        for debt in debts:
            result += f"- **{debt['area']}** (ÂÇµÂãôÁ¥öÂà•: {debt['debt_level']})\n"
            result += f"  - ÂΩ±Èüø: {debt['impact']}\n"
            result += f"  - Âª∫Ë≠∞: {debt['recommendation']}\n"
        return result

    def _format_bottlenecks(self, bottlenecks: List[Dict[str, Any]]) -> str:
        result = ""
        for bottleneck in bottlenecks:
            result += f"- **{bottleneck['bottleneck']}**\n"
            result += f"  - ÂΩ±Èüø: {bottleneck['impact']}\n"
            result += f"  - È†êË®àÊîπÂñÑ: {bottleneck['estimated_improvement']}\n"
        return result

    def _format_code_quality(self, quality: Dict[str, Any]) -> str:
        result = "### ÊúÄ‰Ω≥ÂØ¶Ë∏ê\n"
        result += self._format_list(quality["best_practices"]) + "\n\n"
        result += "### Ë≥™ÈáèÊåáÊ®ô\n"
        for metric, value in quality["quality_metrics"].items():
            result += f"- {metric}: `{value}`\n"
        result += "\n### ÊîπÈÄ≤È†òÂüü\n"
        result += self._format_list(quality["improvement_areas"])
        return result

    def _format_testing_strategy(self, strategy: Dict[str, Any]) -> str:
        result = ""
        result += f"- Ê∏¨Ë©¶Â±§Á¥ö: {', '.join(strategy['test_levels'])}\n"
        result += "### Ë¶ÜËìãÁéá\n"
        for level, coverage in strategy.get("coverage", {}).items():
            result += f"- {level}: {coverage}\n"
        result += f"\n- Ëá™ÂãïÂåñÁ®ãÂ∫¶: {strategy.get('automation_level', '')}\n"
        result += "### ÊîπÈÄ≤Ê©üÊúÉ\n"
        result += self._format_list(strategy.get("improvement_opportunities", []))
        return result

    def _format_ci_cd(self, pipeline: Dict[str, Any]) -> str:
        result = ""
        result += f"- ÊµÅÁ®ãÈöéÊÆµ: {', '.join(pipeline['stages'])}\n"
        result += f"- ‰ΩøÁî®Â∑•ÂÖ∑: {', '.join(pipeline['tools'])}\n"
        result += f"- ÈÉ®ÁΩ≤Á≠ñÁï•: {pipeline['deployment_strategy']}\n"
        result += "### ÊîπÈÄ≤Âª∫Ë≠∞\n"
        result += self._format_list(pipeline.get("improvement_suggestions", []))
        return result


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="MachineNativeOps GitHub Project Deep Analyzer")
    parser.add_argument(
        "--owner",
        default=os.environ.get("GITHUB_REPO_OWNER"),
        help="GitHub repository owner (or set GITHUB_REPO_OWNER)",
    )
    parser.add_argument(
        "--repo",
        default=os.environ.get("GITHUB_REPO_NAME"),
        help="GitHub repository name (or set GITHUB_REPO_NAME)",
    )
    parser.add_argument(
        "--token",
        default=os.environ.get("GITHUB_TOKEN"),
        help="GitHub token to increase rate limits (or set GITHUB_TOKEN)",
    )
    parser.add_argument("--output", choices=["markdown", "json"], default="markdown")
    args = parser.parse_args()
    if not args.owner or not args.repo:
        parser.error("Repository owner and name are required via --owner/--repo or environment variables.")
    return args


def main() -> None:
    if not logging.getLogger().handlers:
        logging.basicConfig(level=logging.INFO)

    args = _parse_args()
    config = GitHubAnalyzerConfig(
        repo_owner=args.owner,
        repo_name=args.repo,
        output_format=args.output,
        token=args.token,
    )
    analyzer = GitHubProjectAnalyzer(config)
    analysis = analyzer.analyze_project()

    if args.output == "json":
        print(json.dumps(analysis, ensure_ascii=False, indent=2))
    else:
        print(analyzer.generate_markdown_report(analysis))


if __name__ == "__main__":
    main()
